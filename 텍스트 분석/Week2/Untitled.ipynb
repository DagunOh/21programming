{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "===========\n",
    "\n",
    "#### 불용어,불필요한 수식, 특수기호, 문자, 숫자 모두 제거해서 깔끔하게 만들어주는 작업.\n",
    "\n",
    "--------\n",
    "## *영어*\n",
    "#### `regex` 정규식 사용\n",
    "\n",
    "|pattern 종류|설명|\n",
    "|---------------|----------------|\n",
    "|[0-9]|숫자만|\n",
    "|[a-zA-Z]|영어만|\n",
    "|[aeiou]|영어모음만|\n",
    "|[.!,?;]|문장부호|\n",
    "|[a‐zA‐Z!@#$%^&*0‐9]|영어,특수기호,숫자|\n",
    "|[^a‐zA‐Z]| 영어 제외하고|\n",
    "|A*|0개 이상|\n",
    "|A+|1개 이상|\n",
    "|A{m,n}|m개 이상 n개 이하|\n",
    "|?|앞에 문자가 존재할 수도 있고, 아닐 수도 있음|\n",
    "\n",
    "**example**\n",
    "\n",
    "서울특별시수처리과 022214‐8107   \n",
    "서울특별시 도시관리국 02 731‐0308    \n",
    "서울은평경찰서 02 389‐0384   \n",
    "경찰청 서울송파경찰서 02 449‐0370   \n",
    "경찰청 서울송파경찰서 02 448‐0365   \n",
    "\n",
    "\n",
    "**code**\n",
    "\n",
    "    import re\n",
    "    re = '02 [0-9]{3,4}-[0-9]{4}'\n",
    "    lst = re.findall(regex pattern, text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "#### `정규 표현식 모듈 함수`\n",
    "\n",
    "|모듈 함수 종류|설명|\n",
    "|---------------|----------------|\n",
    "|re.compile()|정규표현식을 컴파일하는 함수|\n",
    "|re.search()|문자열 전체에 대해서 정규표현식과 매치되는지를 검색|\n",
    "|re.match()|문자열의 처음이 정규표현식과 매치되는지 검색|\n",
    "|re.split()|정규 표현식을 기준으로 문자열을 분리하여 리스트로 리턴|\n",
    "|re.findall()|문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 찾아서 리스트로 리턴|\n",
    "|re.sub()|문자열에서 정규 표현식과 일치하는 부분에 대해서 다른 문자열로 대체|\n",
    "\n",
    "**code**\n",
    "\n",
    "    import re\n",
    "    text = \"\"\"OLIVE DES OLIVE Acoustic한 감성을 바탕으로 couture적인 detail을 넣어 feminine함을 세련되고 art적인 느낌으로 표현합니다\n",
    "    LOTTE\"\"\"\n",
    "\n",
    "    pattern = '[가-힣]+'\n",
    "    lst = re.findall(pattern,text)\n",
    "\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "#### `string 자체의 메소드`\n",
    "\n",
    "|메소드 종류|설명|\n",
    "|---------------|----------------|\n",
    "|str.replace(pattern,alternative)|해당 패턴과 매치되는 부분을 다른 문자열로 대체|\n",
    "|str.lower()|문자열 전체에 대해서 소문자|\n",
    "|str.upper()|문자열 전체에 대해서 대문자|\n",
    "|str.strip()|문자열 공백 제거|\n",
    "|str.split(sep='구분자')|문자열 구분자 기준으로 나누기|\n",
    "|str.isalpha()|문자열이 영어로만 이루어졌으면 True 반환|\n",
    "|str.isnumeric()|문자열이 숫자만 포함하고 있으면 True 반환|\n",
    "\n",
    "**code**\n",
    "\n",
    "    string = \"I am a boy\"\n",
    "    string = string.replace('a','b')\n",
    "    string\n",
    "\n",
    "--------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\"사과 딸기 수박 메론 바나나\"\n",
    "re.split(\" \",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '1234', '30']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\" 이름 : 김철수 , 전화번호 : 010 - 1234 - 1234, 나이 : 30 ,성별 : 남\"\n",
    "    \n",
    "re.findall(\"\\d+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "re.sub('[^a-zA-Z]',' ',text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## *한글*\n",
    "\n",
    "위의 것들을 사용해도 되지만, 전처리 패키지 존재\n",
    "\n",
    "### `PyKoSpacing` 패키지 \n",
    "\n",
    "\n",
    "PyKoSpacing은 한국어 띄어쓰기 패키지로 *띄어쓰기가 되어있지 않은 문장을 띄어쓰기를 한 문장으로 변환해주는 패키지*\n",
    "\n",
    "PyKoSpacing은 대용량 코퍼스를 학습하여 만들어진 띄어쓰기 딥 러닝 모델로 준수한 성능을 가지고 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"김철수는극중두인격의사나이이광수역을맡았다.철수는한국유일의태권도전승자를가리는결전의날을앞두고10년간함께훈련한사형인유연재(김광수분)를찾으러속세로내려온인물이다.\n"
     ]
    }
   ],
   "source": [
    "sent = \"\"\"\"김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\"\"\"\n",
    "\n",
    "new_sent = sent.replace(\" \", '') # 띄어쓰기가 없는 문장 임의로 만들기\n",
    "print(new_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "\"김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n"
     ]
    }
   ],
   "source": [
    "from pykospacing import spacing\n",
    "\n",
    "kospacing_sent = spacing(new_sent)\n",
    "print(sent)\n",
    "print(kospacing_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "### `Py-Hanspell` 패키지 \n",
    "\n",
    "네이버 한글 맞춤법 검사기를 바탕으로 만들어진 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ssut/py-hanspell.git\n",
      "  Cloning https://github.com/ssut/py-hanspell.git to /private/var/folders/m3/zgkmtc792859mlg2v7cnmpy00000gn/T/pip-req-build-riotmq97\n",
      "Requirement already satisfied: requests in /Users/dagunoh/opt/anaconda3/lib/python3.8/site-packages (from py-hanspell==1.1) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/dagunoh/opt/anaconda3/lib/python3.8/site-packages (from requests->py-hanspell==1.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/dagunoh/opt/anaconda3/lib/python3.8/site-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dagunoh/opt/anaconda3/lib/python3.8/site-packages (from requests->py-hanspell==1.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/dagunoh/opt/anaconda3/lib/python3.8/site-packages (from requests->py-hanspell==1.1) (1.25.9)\n",
      "Building wheels for collected packages: py-hanspell\n",
      "  Building wheel for py-hanspell (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-hanspell: filename=py_hanspell-1.1-py3-none-any.whl size=4854 sha256=1666a77e57069eb2afad27dbe994ad379a369edebe0bfc038036aadb84532001\n",
      "  Stored in directory: /private/var/folders/m3/zgkmtc792859mlg2v7cnmpy00000gn/T/pip-ephem-wheel-cache-l0seqo1a/wheels/3f/a5/73/e4d2806ae141d274fdddaabf8c0ed79be9357d36bfdc99e4b4\n",
      "Successfully built py-hanspell\n",
      "Installing collected packages: py-hanspell\n",
      "Successfully installed py-hanspell-1.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지 네가 모 어쩔 건데\n"
     ]
    }
   ],
   "source": [
    "from hanspell import spell_checker\n",
    "\n",
    "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 니가 모 어쩔건대 \"\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "nltk 에서 지정해놓은 불용어들 (사용하지 않는 단어들 제거하기 위함)\n",
    "\n",
    "**code**\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    string = string.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "                \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#직접 불용어 사전 만들 수도 있음.\n",
    "# stopwords = ['그','이','저','-']\n",
    "# unique_Noun_words = set(Noun_words)\n",
    "# for word in unique_Noun_words:\n",
    "#     if word in stopwords:\n",
    "#         while word in Noun_words: Noun_words.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization\n",
    "============\n",
    "\n",
    "### 단어 토큰화 및 품사 태깅 실습 Code\n",
    "> #### 영어\n",
    ">> 영어의 경우, nltk 를 이용하여 토큰화가 가능합니다. \n",
    ">> NLTK 는 Penn Treebank POS Tags 를 기준으로 품사 태깅을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda update nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dagunoh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "x = word_tokenize(text)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dagunoh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pos_tag(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Penn Treebank\n",
    "\n",
    "|Number|Tag|Description|\n",
    "|------|---|-----------|\n",
    "|1.|CC |Coordinating conjunction|\n",
    "|2.|CD |Cardinal number|\n",
    "|3.|DT |Determiner|\n",
    "|4.|EX |Existential there|\n",
    "|5.|FW |Foreign word|\n",
    "|6.|IN |Preposition or subordinating conjunction|\n",
    "|7.|JJ |Adjective|\n",
    "|8.|JJR|Adjective, comparative|\n",
    "|9.|JJS|Adjective, superlative|\n",
    "|10.|LS|List item marker|\n",
    "|11.|MD|Modal|\n",
    "|12.|NN|Noun, singular or mass|\n",
    "|13.|NNS|Noun, plural|\n",
    "|14.|NNP|Proper noun, singular|\n",
    "|15.|NNPS|Proper noun, plural|\n",
    "|16.|PDT|Predeterminer|\n",
    "|17.|POS|Possessive ending|\n",
    "|18.|PRP|Personal pronoun|\n",
    "|19.|PRP\\\\$|Possessive pronoun|\n",
    "|20.|RB |Adverb|\n",
    "|21.|RBR|Adverb, comparative|\n",
    "|22.|RBS|Adverb, superlative|\n",
    "|23.|RP\t|Particle|\n",
    "|24.|SYM|Symbol|\n",
    "|25.|TO\t|to|\n",
    "|26.|UH\t|Interjection|\n",
    "|27.|VB\t|Verb, base form|\n",
    "|28.|VBD|Verb, past tense|\n",
    "|29.|VBG|Verb, gerund or present participle|\n",
    "|30.|VBN|Verb, past participle|\n",
    "|31.|VBP|Verb, non-3rd person singular present|\n",
    "|32.|VBZ|Verb, 3rd person singular present|\n",
    "|33.|WDT|Wh-determiner|\n",
    "|34.|WP\t|Wh-pronoun|\n",
    "|35.|WP$|Possessive wh-pronoun|\n",
    "|36.|WRB|Wh-adverb|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 Code\n",
    "\n",
    ">한국어의 경우 KoNLPy를 활용하여 토큰화가 가능합니다. KoNLPy에서 제공하는 형태소 분석기는 다음과 같습니다.\n",
    "- Okt (Open Korea Text)\n",
    "- 매캡 (Mecab)\n",
    "- 코모란 (Komoran)\n",
    "- 꼬꼬마 (Kkma)\n",
    "- 한나눔 (Hannanum)\n",
    "\n",
    "### **Okt**\n",
    "\n",
    "- `morphs` : 형태소 분석\n",
    "- `pos` : 품사 태깅 (Part-of-speech tagging)\n",
    "- `nouns` : 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['눈', '이', '와요', '눈', '이', '와', '또', '눈', '이', '와']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "text = \"눈이 와요 눈이 와 또 눈이 와\"\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석 결과 출력\n",
    "print(okt.morphs(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('눈', 'Noun'), ('이', 'Josa'), ('와요', 'Verb'), ('눈', 'Noun'), ('이', 'Josa'), ('와', 'Verb'), ('또', 'Noun'), ('눈', 'Noun'), ('이', 'Josa'), ('와', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "# 품사 태깅 결과 출력\n",
    "print(okt.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '은', '눈', '이', '왔습니다', '.', '흰', '눈', '을', '밟고', '조심', '히', '움직여', '보았습니다', '.', '뽀득뽀득', '한', '소리', '가', '마음', '을', '울렸어요', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "text = \"오늘은 눈이 왔습니다. 흰 눈을 밟고 조심히 움직여보았습니다. 뽀득뽀득한 소리가 마음을 울렸어요.\"\n",
    "okt2 = Okt()\n",
    "\n",
    "# 형태소 분석 결과 출력\n",
    "print(okt2.morphs(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('눈', 'Noun'), ('이', 'Josa'), ('왔습니다', 'Verb'), ('.', 'Punctuation'), ('흰', 'Adjective'), ('눈', 'Noun'), ('을', 'Josa'), ('밟고', 'Verb'), ('조심', 'Noun'), ('히', 'Adverb'), ('움직여', 'Verb'), ('보았습니다', 'Verb'), ('.', 'Punctuation'), ('뽀득뽀득', 'Adverb'), ('한', 'Determiner'), ('소리', 'Noun'), ('가', 'Josa'), ('마음', 'Noun'), ('을', 'Josa'), ('울렸어요', 'Verb'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# 품사 태깅 결과 출력\n",
    "print(okt2.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '눈', '눈', '조심', '소리', '마음']\n"
     ]
    }
   ],
   "source": [
    "# 명사만 출력\n",
    "print(okt2.nouns(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "### **꼬꼬마**\n",
    "\n",
    "- `morphs` : 형태소 분석\n",
    "- `pos` : 품사 태깅 (Part-of-speech tagging)\n",
    "- `nouns` : 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['눈', '이', '와요', '눈', '이', '오', '아', '또', '눈', '이', '오', '아']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "text = \"눈이 와요 눈이 와 또 눈이 와\"\n",
    "kkma = Kkma()\n",
    "\n",
    "print(kkma.morphs(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('눈', 'NNG'), ('이', 'JKS'), ('와요', 'NNG'), ('눈', 'NNG'), ('이', 'JKS'), ('오', 'VV'), ('아', 'ECS'), ('또', 'MAG'), ('눈', 'NNG'), ('이', 'JKS'), ('오', 'VA'), ('아', 'ECS')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "#### **세종 품사 태그**\n",
    "\n",
    "|대분류|태그|설명|\n",
    "|----|---|---|\n",
    "|체언|NNG|일반 명사|\n",
    "|체언|NNP|고유 명사|\n",
    "|체언|NNB|의존 명사|\n",
    "|체언|NR |수사|\n",
    "|체언|NP |대명사|\n",
    "|용언|VV |동사|\n",
    "|용언|VA |형용사|\n",
    "|용언|VX |보조 용언|\n",
    "|용언|VCP|긍정 지정사|\n",
    "|용언|VCN|부정 지정사|\n",
    "|관형사|MM|관형사|\n",
    "|부사|MAG|일반 부사|\n",
    "|부사|MAJ|접속 부사|\n",
    "|감탄사|IC|감탄사|\n",
    "|조사|JKS|주격 조사|\n",
    "|조사|JKC|보격 조사|\n",
    "|조사|JKG|관형격 조사|\n",
    "|조사|JKO|목적격 조사|\n",
    "|조사|JKB|부사격 조사|\n",
    "|조사|JKV|호격 조사|\n",
    "|조사|JKQ|인용격 조사|\n",
    "|조사|JX |보조사|\n",
    "|조사|JC |접속 조사|\n",
    "|선어말 어미|EP|선어말 어미|\n",
    "|어말 어미|EF|종결 어미|\n",
    "|어말 어미|EC||연결 어미|\n",
    "|어말 어미|ETN|명사형 전성 어미|\n",
    "|어말 어미|ETM|관형형 전성 어미|\n",
    "|접두사|XPN|체언 접두사|\n",
    "|접미사|XSN|명사 파생 접미사|\n",
    "|접미사|XSV|동사 파생 접미사|\n",
    "|접미사|XSA|형용사 파생 접미사|\n",
    "|어근|XR|어근|\n",
    "|부호|SF|마침표, 물음표, 느낌표|\n",
    "|부호|SP|쉼표, 가운뎃점, 콜론, 빗금|\n",
    "|부호|SS|따옴표, 괄호표, 줄표|\n",
    "|부호|SE|줄임표|\n",
    "|부호|SO|붙임표 (물결, 숨김, 빠짐)|\n",
    "|부호|SW|기타기호 (논리수학기호, 화폐기호)|\n",
    "|분석 불능|NF|명사추정범주|\n",
    "|분석 불능|NV|용언추정범주|\n",
    "|분석 불능|NA|분석불능범주|\n",
    "|한글 이외|SL|외국어|\n",
    "|한글 이외|SH|한자|\n",
    "|한글 이외|SN|숫자|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "실습 - Spam or not\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `데이터 불러오기`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>isSpam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  isSpam\n",
       "0     Go until jurong point, crazy.. Available only ...       0\n",
       "1                         Ok lar... Joking wif u oni...       0\n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3     U dun say so early hor... U c already then say...       0\n",
       "4     Nah I don't think he goes to usf, he lives aro...       0\n",
       "...                                                 ...     ...\n",
       "5567  This is the 2nd time we have tried 2 contact u...       1\n",
       "5568              Will Ì_ b going to esplanade fr home?       0\n",
       "5569  Pity, * was in mood for that. So...any other s...       0\n",
       "5570  The guy did some bitching but I acted like i'd...       0\n",
       "5571                         Rofl. Its true to its name       0\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\", filename=\"spam.csv\")\n",
    "data = pd.read_csv('spam.csv', encoding='latin1')\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "del data['Unnamed: 2']\n",
    "del data['Unnamed: 3']\n",
    "del data['Unnamed: 4']\n",
    "\n",
    "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])   #Ham 이 0, spam 이 1\n",
    "data['text'] = data['v2']\n",
    "data['isSpam'] = data['v1']\n",
    "del data['v1'], data['v2']\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](./IMAGE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(string: str, *args, **kwargs) -> str:\n",
    "    from nltk.stem.porter import PorterStemmer #어간 추출\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    string = data.text\n",
    "    string = string.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
    "                                 'emailaddress') #이메일 한 번에 처리\n",
    "    string = string.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
    "                                  'webaddress') #웹주소 한 번에 처리\n",
    "    string = string.str.replace(r'£|\\$', 'moneysymb') #이런건 왜 나올까\n",
    "    string = string.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
    "                                  'phonenumbr') #휴대폰 번호 한 번에 처리\n",
    "    string = string.str.replace('[^a-zA-Z]', ' ') #영어 말고다 버려\n",
    "    string= string.str.lower() #소문자\n",
    "    \n",
    "    ##불용어 제거\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    string = string.apply(lambda x: ' '.join(word for word \n",
    "                                             in x.split() if word not in stop_words))\n",
    "    \n",
    "    ###어간만!              \n",
    "    ps = PorterStemmer()\n",
    "    final_processed = string.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))\n",
    "    \n",
    "    print(final_processed)\n",
    "    return final_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       go jurong point crazi avail bugi n great world...\n",
      "1                                   ok lar joke wif u oni\n",
      "2       free entri wkli comp win fa cup final tkt st m...\n",
      "3                     u dun say earli hor u c alreadi say\n",
      "4                    nah think goe usf live around though\n",
      "                              ...                        \n",
      "5567    nd time tri contact u u moneysymb pound prize ...\n",
      "5568                                b go esplanad fr home\n",
      "5569                                    piti mood suggest\n",
      "5570    guy bitch act like interest buy someth els nex...\n",
      "5571                                       rofl true name\n",
      "Name: text, Length: 5572, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'go jurong point crazi avail bugi n great world la e buffet cine got amor wat'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_processed = preprocess(data.text)\n",
    "final_processed[0] #check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tokenizing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string: str, *args, **kwargs) -> list:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    document = [] \n",
    "    word_docx = []\n",
    "    for mails in final_processed:\n",
    "        words = word_tokenize(mails)\n",
    "        word_docx.append(words)\n",
    "        for i in words:\n",
    "            document.append(i)  \n",
    "            \n",
    "    return document, word_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document, word_docx = tokenize(final_processed)\n",
    "#tokenize(final_processed)[:10]\n",
    "len(word_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document, word_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in '/Users/dagunoh/nltk_data/corpora/stopwords'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
